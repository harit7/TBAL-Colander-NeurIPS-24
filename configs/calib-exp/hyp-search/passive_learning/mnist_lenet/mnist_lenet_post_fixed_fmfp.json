
{
    "post_hoc": 
    {
        "global_calib_params" : 
        { 
            "calib_val_frac" : [ 0.5]
        },
        
        "top_lbl_hb_params" : 
        { 
            "points_per_bin": [50,25]
        },
        "scaling_params" :     
        {
            "training_conf.optimizer" : ["adam"],
            "training_conf.learning_rate": [0.5],
            "training_conf.batch_size" : [64],
            "training_conf.max_epochs": [20],
            "training_conf.weight_decay": [1.0]
        },
        "scaling_binning_params":
        {
            "training_conf.num_bins": [10,20],
            "training_conf.learning_rate": [0.5],
            "training_conf.batch_size" : [64],
            "training_conf.max_epochs": [20],
            "training_conf.weight_decay": [1.0]
        },

        "dirichlet_params":
        {
            "training_conf.optimizer" : ["adam"],
            "training_conf.learning_rate" : [0.5],
            "training_conf.reg" : [1e-2],
            "training_conf.batch_size" : [64],
            "training_conf.max_epochs": [20]
        },

        "auto_lbl_opt_v0_params" : 
        {
            "l1" : [1.0],
            "l2" : [100.0,1000.0],
            "l3" : [0.0],
            "features_key" : ["concat","pre_logits","logits"], 
            "class_wise" : ["independent"],
            
            "training_conf_g.optimizer" : ["adam"],
<<<<<<< Updated upstream:configs/calib-exp/hyp-search/passive_learning/mnist_lenet/mnist_lenet_post_fixed_fmfp.json
            "training_conf_g.learning_rate": [0.001], 
            "training_conf_g.max_epochs": [1000,2000],
            "training_conf_g.weight_decay": [1.0],
=======
            "training_conf_g.learning_rate": [0.1,0.01], 
            "training_conf_g.max_epochs": [1000,2000],
            "training_conf_g.weight_decay": [1.0, 0.1,0.01],
>>>>>>> Stashed changes:configs/calib-exp/hyp-search/mnist_lenet/mnist_lenet_post_search.json
            "training_conf_g.batch_size": [100],
            "regularize": [true],
            "alpha_1" : [0.001],
            "model_conf":["two_layer : 2 : tanh"] 
        }   
    }
}



